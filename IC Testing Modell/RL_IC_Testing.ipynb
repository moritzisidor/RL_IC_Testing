{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3ef4018",
   "metadata": {},
   "source": [
    "## Note:\n",
    "#### Make sure to meet all requirements of requirements.txt in advance.\n",
    "#### To train the Agent, you may skip sections 0,1,2.\n",
    "Sections 0, 1 and 2 only demonstrate how the virtual test environment was implemented in the package 'gymnasium-custom'.\\\n",
    "In section 2, the import and functionality of the package can be tested by means of a simulation.\n",
    "\n",
    "Section 3 can be run independently of sections 1 and 2. It includes the reinforcement learning model and the training process of the agent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa6fedcf",
   "metadata": {},
   "source": [
    "# 0. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4831cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym # 0.27.1\n",
    "from gymnasium import logger, spaces\n",
    "import pygame # 2.3.0\n",
    "import numpy as np # 1.22.4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e4d7f7b",
   "metadata": {},
   "source": [
    "# 1. Setup Custom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "import pandas as pd\n",
    "import pyreadr as pr\n",
    "from dotenv import dotenv_values\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym # 0.27.1\n",
    "from gymnasium import logger, spaces\n",
    "import pygame # 2.3.0\n",
    "import numpy as np # 1.22.4\n",
    "import math\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "import pandas as pd\n",
    "import pyreadr as pr\n",
    "from dotenv import dotenv_values\n",
    "from os import path\n",
    "\n",
    "\n",
    "class IcTestEnvironment(gym.Env):\n",
    "    \"\"\"\n",
    "    Description ...\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "    \n",
    "    def __init__(self, render_mode=None, data=False):\n",
    "        \n",
    "        if data:\n",
    "            self.data_path = data\n",
    "        else:\n",
    "            env_variables = dotenv_values('.env')\n",
    "            file_name = env_variables['TRAINING_FILE']\n",
    "            file_path = env_variables['DATA_PATH']\n",
    "            self.data_path = path.join(file_path, file_name)\n",
    "            \n",
    "        self.data = pd.read_csv(self.data_path, delimiter = \";\").to_numpy() # read .csv file, convert pd.df to np.array\n",
    "        self.test_data = self.data[0:, 8:] # slice array to relevant test data\n",
    "        self.cond_label = self.data[0:,0] # slice array to IC-condition labels (1: good device, other: bad device)\n",
    "        \n",
    "        \n",
    "        self.no_of_tests = np.shape(self.test_data)[1] # ammount of tests in data\n",
    "        self.no_of_duts = len(self.cond_label) # ammount of DUTs in data\n",
    "        self.test_no = 0 # initial Test No.\n",
    "        self.dut_id = -1 # initial DUT ID\n",
    "        \n",
    "        self.dut_cond = None # Agent based IC Condition, True: good device, False: bad device\n",
    "        self.true_dut_cond = None # True IC Condition, True: good device, False: bad device\n",
    "        \n",
    "        # State space: [Test No., Test Result]\n",
    "        # low: -Inf\n",
    "        states_low = [-np.finfo(np.float32).max] * self.no_of_tests\n",
    "        low = np.array(states_low, dtype=np.float32)\n",
    "        # high: Inf\n",
    "        states_high = [np.finfo(np.float32).max] * self.no_of_tests\n",
    "        high = np.array(states_high, dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)\n",
    "        \n",
    "        self.state = None\n",
    "        \n",
    "        # 3 actions corresponding to 0: \"abort good\", 1: \"abort bad\", 2: \"continue\"\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        \n",
    "        # define instant reward (to be used in reward functions)\n",
    "        self.inst_reward = 0\n",
    "        \n",
    "        # In case of process visualization:\n",
    "        self.render_mode = render_mode\n",
    "        self.screen_width = 600\n",
    "        self.screen_height = 400\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "               \n",
    "        # Ammount of steps after a terminating state\n",
    "        self.steps_beyond_terminated = None\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        This method defines both reward and next state in dependence of an action taken by the agent.\n",
    "        Returns: the next state, the reward, whether the episode is terminated or not and optionally additional info\n",
    "        \"\"\"\n",
    "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "        assert self.state is not None, \"Call reset before using step method.\" \n",
    "        \n",
    "        self.test_no +=1\n",
    "        \n",
    "        if action == 2:\n",
    "            test_no = float(self.test_no)\n",
    "            test_result = self.test_data[self.dut_id, self.test_no]\n",
    "            \n",
    "        elif action == 1:\n",
    "            test_no = float(self.test_no)\n",
    "            test_result = 0 \n",
    "            self.dut_cond = False # bad device\n",
    "            \n",
    "        elif action == 0:\n",
    "            test_no = float(self.test_no)\n",
    "            test_result = 0 \n",
    "            self.dut_cond = True # good device\n",
    "        \n",
    "        # overwrite state\n",
    "        self.state[self.test_no] = test_result\n",
    "        \n",
    "        overdue = (self.test_no == (self.no_of_tests-1))\n",
    "\n",
    "        terminated = bool(action == 1 or action == 0 or overdue)\n",
    "        \n",
    "        # Reward Functions:\n",
    "        #tbr = np.tanh(2e-3*self.test_no) - 8e-4 * self.test_no # tbr V1\n",
    "        tbr = -1e-1 # tbr V2\n",
    "\n",
    "        if not terminated:\n",
    "            reward = tbr\n",
    "        \n",
    "        elif self.steps_beyond_terminated is None:\n",
    "            \n",
    "            self.steps_beyond_terminated = 0\n",
    "\n",
    "            if self.cond_label[self.dut_id] == 1:\n",
    "                self.true_dut_cond = True\n",
    "\n",
    "            elif self.cond_label[self.dut_id] != 1:\n",
    "                self.true_dut_cond = False\n",
    "\n",
    "            print(\"DUT No.: {}\".format(self.dut_id), end = '\\r', flush = True)\n",
    "\n",
    "            if self.dut_cond == None:\n",
    "                reward = -800\n",
    "                \n",
    "            elif self.dut_cond == False and not self.true_dut_cond:\n",
    "                reward = 250*np.tanh(2e-3*self.test_no - 1.5) + 250\n",
    "                \n",
    "            elif self.dut_cond == True and self.true_dut_cond:\n",
    "                reward = 250*np.tanh(2e-3*self.test_no - 1.5) + 250\n",
    "                \n",
    "            else:\n",
    "                reward = -300 * np.tanh(2e-3*self.test_no - 1.5) - 400\n",
    "        \n",
    "        else:\n",
    "            if self.steps_beyond_terminated == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned terminated = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'terminated = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "                \n",
    "            self.steps_beyond_terminated += 1\n",
    "            reward = 0.0\n",
    "            \n",
    "        return np.array(self.state, dtype=np.float32), reward, terminated, False, {'PC' : self.dut_cond, 'TC' : self.true_dut_cond}\n",
    "    \n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Resets to the initial state (Test 0, Test Result 0, pending test results defined as infinite).\n",
    "        Calling the method indicates, that the testing process proceeds by testing the next DUT.\n",
    "        Returns the initial state.\n",
    "        \"\"\"\n",
    "\n",
    "        # call reset of parent class Env in core.py\n",
    "        super().reset(seed=seed) \n",
    "        \n",
    "        # reset attributes \n",
    "        self.dut_cond = None # set condition to unknown\n",
    "        self.true_dut_cond = None # set true condition to unknown\n",
    "        \n",
    "        if self.dut_id == (self.no_of_duts-1): # Out of data (end of epoch), reset to DUT 0\n",
    "            self.dut_id = 0\n",
    "        else:\n",
    "            self.dut_id += 1 # next DUT\n",
    "            \n",
    "        self.test_no = 0 # reset to test 0\n",
    "        \n",
    "        # set intial state: test result 0 + rest of list filled with value 1 * (no_of_tests - 1) \n",
    "        self.state = [self.test_data[self.dut_id, 0]] + [1] * (self.no_of_tests-1)\n",
    "        self.steps_beyond_terminated = None\n",
    "        \n",
    "        return np.array(self.state, dtype=np.float32), {}\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Mandatory, yet unused in the case of IC testing.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1acfbd33",
   "metadata": {},
   "source": [
    "## 1.1 Test Functionality of Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f830babf",
   "metadata": {},
   "source": [
    "Make sure to add the DATA_PATH and a TRAINING_FILE name to your .env file. The .env file has to be located in the same folder as this Notebook.\\\n",
    "If the environment variables are not to be used, a data path to the training data set can be specified directly via the argument \"data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ebf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ICenv = IcTestEnvironment() # or data = 'training_data_path' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5bc3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ICenv.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2352):\n",
    "    ICenv.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6cea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "ICenv.step(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "843d81a3",
   "metadata": {},
   "source": [
    "# 2. Import Custom Environment and Simulation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf404e21",
   "metadata": {},
   "source": [
    "#### Requirement:\n",
    "Package 'gymnasium-custom' is installed locally and path/file informations are added to .env file, located in the same folder as this Notebook.\\\n",
    "Alternatively, a path can be specified with the argument 'data' in gym.make()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium_custom\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75adb5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ICTesting-v0\") # if environment variables are not to be used: add argument data = 'training_data_path'\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb01f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    count = 0\n",
    "    time.sleep(1)\n",
    "    \n",
    "    while not done:\n",
    "        env.render() \n",
    "        \n",
    "        action = np.random.choice(np.arange(0, 3), p=[0.05, 0.05, 0.9])\n",
    "\n",
    "        \n",
    "        # env.step to process one step -> return: next state, reward, done: T or F, info\n",
    "        n_state, reward, done, info, info2  = env.step(action) \n",
    "        \n",
    "        # Reward +1\n",
    "        score += reward\n",
    "        count += 1\n",
    "        if done:\n",
    "            if action == 0:\n",
    "                decision = 'good device'\n",
    "            elif action == 1:\n",
    "                decision = 'bad device'\n",
    "            elif action == 2:\n",
    "                decision = 'no decision'\n",
    "    \n",
    "    print('Episode: {} # of Tests: {} Decision: {} Score: {}'.format(episode, count, decision, score))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c240515f",
   "metadata": {},
   "source": [
    "# 3. Reinforcement Learning Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9359dc4",
   "metadata": {},
   "source": [
    "#### Requirement:\n",
    "requirements.txt are installed.\\\n",
    "Package 'gymnasium-custom' is installed locally and path/file informations are added to your .env file. The .env file has to be located in the same folder as this Notebook.\\\n",
    "Alternatively, a path to your training data can be specified with the argument 'data' in gym.make()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5b59b08",
   "metadata": {},
   "source": [
    "## 3.1 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86127dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_custom\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7478f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load custom environment\n",
    "env = gym.make(\"ICTesting-v0\") # if environment variables are not to be used: add argument data = 'training_data_path'\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# check if GPU supports cuda. Else cpu is used.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "147bf819",
   "metadata": {},
   "source": [
    "## 3.2 Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877da2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    Stores the transitions that the agent observes. The data is to be reused for training purposes.\n",
    "    By sampling from it randomly, the transitions that build up a batch are decorrelated.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e42c0eb",
   "metadata": {},
   "source": [
    "## 3.3 DQN Agent - Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7700a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 2356)\n",
    "        self.layer2 = nn.Linear(2356, 1180)\n",
    "        self.layer3 = nn.Linear(1180, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[abortgood0exp, abortbad0exp, continue0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "# to understand Input/Output of a DQN\n",
    "net = DQN(10,3)\n",
    "input = torch.randn(1, 10)\n",
    "print(input)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a21f636c",
   "metadata": {},
   "source": [
    "## 3.4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b0993",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128 # batch of states\n",
    "GAMMA = 0.99 # discount factor to compute the discounted total reward\n",
    "\n",
    "# €-greedy algorithm params:\n",
    "EPS_START = 0.05 # starting epsilon (decreasing exponentially)\n",
    "EPS_END = 0.0 # ending epsilon \n",
    "EPS_DECAY = 6e05 #controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "\n",
    "TAU = 0.005 # update rate of target network\n",
    "LR = 1e-04 # Learning rate for Adam Optimizer\n",
    "lmbd = 0.9 # regularization rate\n",
    "\n",
    "n_actions = env.action_space.n # number of actions: 3\n",
    "state, info = env.reset() # initial state\n",
    "n_observations = len(state) # number of state observations: depends on test data\n",
    "min_test_ammount = 10 # minimum ammount of tests per DUT\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True) # momentum-based optimization (based on gradient descent)\n",
    "memory = ReplayMemory(30000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "# €-greedy Algorithm:\n",
    "\n",
    "def select_action(state, t):\n",
    "    '''\n",
    "    returns action (as tensor) given the current state\n",
    "    e.g. a = [[1]]\n",
    "    '''\n",
    "    global steps_done\n",
    "    global min_test_ammount\n",
    "    steps_done += 1\n",
    "\n",
    "    if t <= min_test_ammount:\n",
    "        return None, torch.tensor([[2]], device=device, dtype=torch.long)\n",
    "    else:\n",
    "        sample = random.random() # Unif([0,1])\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "        if eps_threshold < 1.5e-03:\n",
    "            eps_threshold = 0\n",
    "            \n",
    "        if sample > eps_threshold: # pick best action under current model\n",
    "            with torch.no_grad():\n",
    "                # t.max(1) will return the largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward. (see 3.3 class DQN)\n",
    "                return eps_threshold, policy_net(state).max(1)[1].view(1, 1)\n",
    "        #elif sample > .3*eps_threshold:\n",
    "        #    return eps_threshold, torch.tensor([[2]], device=device, dtype=torch.long) # continue testing action 2\n",
    "        else: # pick action uniformly at random among all actions\n",
    "            return eps_threshold, torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration (# of tests)')\n",
    "    plt.ylim(min(episode_durations),max(episode_durations))\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy(), 'tab:orange')\n",
    "\n",
    "    plt.pause(0.001)  # pause to update plots\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "\n",
    "episode_reward = []\n",
    "episode_epsilon = []\n",
    "\n",
    "\n",
    "def plot_reward(show_result=False):\n",
    "    plt.figure(1)\n",
    "    reward_t = torch.tensor(episode_reward, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    if torch.cuda.is_available():\n",
    "        ylim_low = min(episode_reward).cpu().numpy()[0]\n",
    "        ylim_high = max(episode_reward).cpu().numpy()[0]+100\n",
    "    else:\n",
    "        ylim_low = min(episode_reward)\n",
    "        ylim_high = max(episode_reward)\n",
    "    \n",
    "    plt.ylim(ylim_low,ylim_high)\n",
    "    plt.plot(reward_t.numpy(), 'g')\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(reward_t) >= 100:\n",
    "        means = reward_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy(), 'tab:orange')\n",
    "        \n",
    "    # Plot Epsilon:\n",
    "    eps_list = [i * ylim_high for i in episode_epsilon]\n",
    "    plt.plot(eps_list, 'red')\n",
    "    \n",
    "\n",
    "    plt.pause(0.001)  # pause to update plots\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "episode_loss = []\n",
    "\n",
    "def plot_loss(show_result=False):\n",
    "    loss_t = torch.tensor(episode_loss, dtype=torch.float)\n",
    "    if len(loss_t) >= 100:\n",
    "        plt.figure(1)\n",
    "        if show_result:\n",
    "            plt.title('Result')\n",
    "        else:\n",
    "            plt.clf()\n",
    "            plt.title('Training...')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Mean Loss')\n",
    "        plt.ylim(min(episode_loss),max(episode_loss[30:]))\n",
    "        plt.plot(loss_t.numpy(), 'm')\n",
    "        # Take 100 episode averages and plot them too\n",
    "        means = loss_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy(), 'tab:orange')\n",
    "\n",
    "    plt.pause(0.001)  # pause to update plots\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "\n",
    "\n",
    "def plot_results(path='add_default_path', window=100):\n",
    "\n",
    "    d = episode_durations\n",
    "    average_d = []\n",
    "    for ind in range(len(d) - window + 1):\n",
    "        average_d.append(np.mean(d[ind:ind+window]))\n",
    "    for ind in range(window - 1):\n",
    "        average_d.insert(0, np.nan)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration (# of Tests)')\n",
    "    plt.ylim(min(episode_durations),max(episode_durations))\n",
    "    plt.plot(d, 'b')\n",
    "    plt.plot(average_d, 'tab:orange', label='Moving average')\n",
    "    plt.grid(linestyle=':')\n",
    "    plt.legend()\n",
    "    fig_l = plt.gcf()\n",
    "    plt.show()\n",
    "    if path:\n",
    "        p = path + '/Duration_' + time.strftime(\"%Y%m%d-%H%M%S\") + '.pdf'\n",
    "        fig_l.savefig(p, dpi=100)\n",
    "    \n",
    "\n",
    "    r = torch.tensor(episode_reward, dtype=torch.float).numpy()\n",
    "    average_r = []\n",
    "    for ind in range(len(r) - window + 1):\n",
    "        average_r.append(np.mean(r[ind:ind+window]))\n",
    "    for ind in range(window - 1):\n",
    "        average_r.insert(0, np.nan)\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    color = 'g'\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Reward', color=color)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        ylim_low = min(episode_reward).cpu().numpy()[0]\n",
    "        ylim_high = max(episode_reward).cpu().numpy()[0]+100\n",
    "    else:\n",
    "        ylim_low = min(episode_reward)\n",
    "        ylim_high = max(episode_reward)\n",
    "        \n",
    "    ax1.set_ylim(ylim_low, ylim_high)    \n",
    "    ax1.plot(r, color=color)\n",
    "    ax1.plot(average_r, color='tab:orange', label='Moving Average')\n",
    "    ax1.grid(linestyle=':')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    \n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Epsilon', color=color)\n",
    "    ax2.set_ylim(EPS_END, EPS_START + 0.05)\n",
    "    ax2.plot(episode_epsilon, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig_r = plt.gcf()\n",
    "    plt.show()\n",
    "    if path:\n",
    "        p = path + '/Reward_' + time.strftime(\"%Y%m%d-%H%M%S\") + '.pdf'\n",
    "        fig_r.savefig(p, dpi=100)\n",
    "    \n",
    "\n",
    "    l = episode_loss\n",
    "    average_l = []\n",
    "    for ind in range(len(l) - window + 1):\n",
    "        average_l.append(np.mean(l[ind:ind+window]))\n",
    "    for ind in range(window - 1):\n",
    "        average_l.insert(0, np.nan)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(min(episode_loss),max(episode_loss[50:]))\n",
    "    plt.plot(l, 'm')\n",
    "    plt.plot(average_l, 'tab:orange', label='Moving average')\n",
    "    plt.grid(linestyle=':')\n",
    "    plt.legend()\n",
    "    fig_r = plt.gcf()\n",
    "    plt.show()\n",
    "    if path:\n",
    "        p = path + '/Loss_' + time.strftime(\"%Y%m%d-%H%M%S\") + '.pdf'\n",
    "        fig_r.savefig(p, dpi=100)\n",
    "\n",
    "# Confusion Matrix Count:\n",
    "ND = 0 # No Decision\n",
    "TP = 0 # True Positive\n",
    "FP = 0 # False Positive\n",
    "FN = 0 # False Negative\n",
    "TN = 0 # True Negative\n",
    "\n",
    "def performance():\n",
    "    global ND, TP, FP, FN, TN\n",
    "    cm = np.matrix([[TP, FP],[FN, TN]])\n",
    "    try:\n",
    "        acc = (TP+TN) / (TP+TN+FP+FN)\n",
    "        print('Confusion Matrix: \\n')\n",
    "        print(cm, '\\n')\n",
    "        print('Accuracy: ', acc)\n",
    "    except:\n",
    "        print(\"An error occured: No DUT has been classified\")\n",
    "\n",
    "def save_agent(path):\n",
    "    \"\"\"\n",
    "    Saves weights of current policy_net\n",
    "    \"\"\"\n",
    "    p = path + '/Agent_' + time.strftime(\"%Y%m%d-%H%M%S\") + '.pth'\n",
    "    torch.save(policy_net.state_dict(), p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edfba3ab",
   "metadata": {},
   "source": [
    "### 3.4.1 Optimization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(reg=False):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return # Memory not yet large enough\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch. This converts batch-array of Transitions to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                       if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "        \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1)) \n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    ## L2 Regularization:\n",
    "    if reg:\n",
    "        for param in policy_net.parameters():\n",
    "            reg_loss = 0.5 * torch.sum(param**2) # euclidian norm of weights\n",
    "        \n",
    "        loss += lmbd * reg_loss\n",
    "        \n",
    "    loss_val = loss.item()\n",
    "    \n",
    "    ## Compute gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    ## In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa90249d",
   "metadata": {},
   "source": [
    "### 3.4.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6930e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(episodes=50, epochs=1):\n",
    "\n",
    "    num_epochs = epochs\n",
    "    num_episodes = episodes\n",
    "\n",
    "    for i_epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for i_episode in range(num_episodes):\n",
    "            # Initialize the environment and get it's state\n",
    "            state, info = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            total_reward = 0\n",
    "            total_loss = 0\n",
    "\n",
    "            for t in count():\n",
    "                E_t, action = select_action(state, t)\n",
    "                observation, reward, terminated, truncated, condition = env.step(action.item())\n",
    "                reward = torch.tensor([reward], device=device)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                total_reward += reward.float()\n",
    "\n",
    "                if terminated:\n",
    "                    next_state = None\n",
    "                else:\n",
    "                    next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "                # Store the transition in memory\n",
    "                memory.push(state, action, next_state, reward)\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network) (Adam Optimizer) and return step loss\n",
    "                step_loss = optimize_model()\n",
    "                if step_loss:\n",
    "                    total_loss += step_loss\n",
    "                    \n",
    "                # Soft update of the target network's weights\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                target_net_state_dict = target_net.state_dict()\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "                target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "                if done:\n",
    "                    # Add predicted [0] and true condition [1] of DUT to global count\n",
    "                    global ND, TP, FP, FN, TN\n",
    "                    if condition['PC'] == None:\n",
    "                        ND += 1\n",
    "                    elif condition['PC'] and condition['TC']:\n",
    "                        TP +=1\n",
    "                    elif condition['PC'] and not condition['TC']:\n",
    "                        FP +=1\n",
    "                    elif not condition['PC'] and condition['TC']:\n",
    "                        FN += 1\n",
    "                    elif not condition['PC'] and not condition['TC']:\n",
    "                        TN += 1\n",
    "                    episode_durations.append(t+1) # append number of Tests of episode\n",
    "                    episode_reward.append(total_reward) # append total reward of episode\n",
    "                    episode_loss.append(total_loss/(t+1)) # append mean loss of episode\n",
    "                    episode_epsilon.append(E_t)\n",
    "                    break\n",
    "\n",
    "        print('Epoch ', i_epoch+1, '/', num_epochs, 'Complete')\n",
    "        if i_epoch == (num_epochs-1):\n",
    "            print('Training complete')\n",
    "            plot_durations(show_result=True)\n",
    "            plot_reward(show_result=True)\n",
    "            plot_loss(show_result=True)\n",
    "            plt.ioff()\n",
    "            plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80a51f60",
   "metadata": {},
   "source": [
    "Attention: A high number of episodes and epochs can lead to high computing time! Especially if GPU is not in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aefa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "training(episodes=90, epochs=1) # Define episodes per epoch and amount of training epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b0a6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c86d7cc",
   "metadata": {},
   "source": [
    "Note: plot_results() works only after an ammount of > 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab58a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(path='C:/Users/username/Results/', window=200) # add path or define default path in function "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14d8dcd1",
   "metadata": {},
   "source": [
    "### 3.4.3 Save Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a7153",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_agent(path='C:/Users/username/Agents/') # add path or define default path in function "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ddb0526",
   "metadata": {},
   "source": [
    "### 3.4.4 Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f8526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(path):\n",
    "    \"\"\"\n",
    "    Saves durations, rewards, epsilons and loss as .txt files\n",
    "    Can be used for further data analysis\n",
    "    \"\"\"\n",
    "    global episode_durations, episode_reward, episode_epsilon, episode_loss\n",
    "    \n",
    "    pathd = path + '/durations_' + time.strftime(\"%Y%m%d-%H%M%S\") + '.txt'\n",
    "    pathr = path + '/rewards_' + time.strftime(\"%Y%m%d-%H%M%S\") + '.txt'\n",
    "    pathe = path + '/epsilons_' + time.strftime(\"%Y%m%d-%H%M%S\") + '.txt'\n",
    "    pathl = path + '/loss_' + time.strftime(\"%Y%m%d-%H%M%S\") + '.txt'\n",
    "    \n",
    "    with open(r'%s' % pathd, 'w') as f:\n",
    "        for d in episode_durations:\n",
    "            f.write(\"%s\\n\" % d)\n",
    "        f.close()\n",
    "        \n",
    "    ep_rew = [r.item() for r in episode_reward]\n",
    "    with open(r'%s' % pathr, 'w') as f:\n",
    "        for r in ep_rew:\n",
    "            f.write(\"%s\\n\" % r)\n",
    "        f.close()\n",
    "\n",
    "    with open(r'%s' % pathe, 'w') as f:\n",
    "        for e in episode_epsilon:\n",
    "            f.write(\"%s\\n\" % e)\n",
    "        f.close()\n",
    "\n",
    "    with open(r'%s' % pathl, 'w') as f:\n",
    "        for l in episode_loss:\n",
    "            f.write(\"%s\\n\" % l)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9421a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(path='C:/Users/username/training_results/') # add path "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14144794",
   "metadata": {},
   "source": [
    "## 3.5 Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da551263",
   "metadata": {},
   "source": [
    "#### Requirement:\n",
    "Agent was trained and saved as .pth file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c38262e",
   "metadata": {},
   "source": [
    "### 3.5.1 Load Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71f2a620",
   "metadata": {},
   "source": [
    "Note: Insert path to agent and path to training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42018a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DQN(n_observations, n_actions).to(device)\n",
    "policy.load_state_dict(torch.load(\"C:/add_path/Agent_example.pth\", map_location=torch.device('cpu')))\n",
    "policy.eval()\n",
    "\n",
    "env_test = gym.make(\"ICTesting-v0\", data = 'C:/Users/username/Data/test.csv')\n",
    "\n",
    "\n",
    "def testing():\n",
    "\n",
    "    lot_size = np.shape(env_test.data)[0]\n",
    "\n",
    "    for dut in range(lot_size):\n",
    "        # Initialize the environment and get it's state\n",
    "        state, info = env_test.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        for t in count():\n",
    "            with torch.no_grad():\n",
    "                action = policy(state).max(1)[1].view(1, 1)\n",
    "            observation, reward, terminated, truncated, condition = env_test.step(action.item())\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            if terminated:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                     \n",
    "                # Add predicted [0] and true condition [1] of DUT to global count\n",
    "                global T_ND, T_TP, T_FP, T_FN, T_TN\n",
    "                if condition['PC'] == None:\n",
    "                    T_ND += 1\n",
    "                elif condition['PC'] and condition['TC']:\n",
    "                    T_TP +=1\n",
    "                elif condition['PC'] and not condition['TC']:\n",
    "                    T_FP +=1\n",
    "                elif not condition['PC'] and condition['TC']:\n",
    "                    T_FN += 1\n",
    "                elif not condition['PC'] and not condition['TC']:\n",
    "                    T_TN += 1\n",
    "                break\n",
    "\n",
    "    print('Testing Complete:\\n')\n",
    "    test_performance()\n",
    "\n",
    "# Confusion Matrix Count:\n",
    "T_ND = 0 # No Decision\n",
    "T_TP = 0 # True Positive\n",
    "T_FP = 0 # False Positive\n",
    "T_FN = 0 # False Negative\n",
    "T_TN = 0 # True Negative\n",
    "\n",
    "def test_performance():\n",
    "    global T_ND, T_TP, T_FP, T_FN, T_TN\n",
    "    cm = np.matrix([[T_TP, T_FP],[T_FN, T_TN]])\n",
    "    try:\n",
    "        acc = (T_TP+T_TN) / (T_TP+T_TN+T_FP+T_FN)\n",
    "        print('Confusion Matrix: \\n')\n",
    "        print(cm, '\\n')\n",
    "        print('Accuracy: ', acc)\n",
    "    except:\n",
    "        print(\"Lazy Agent Error: No DUT has been classified\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c9a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
